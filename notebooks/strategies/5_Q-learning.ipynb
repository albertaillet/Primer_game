{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from game_simulation import CoinGameSimulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 5: make use of Q-learning.\n",
    "This strategy uses Q-learning with an $\\epsilon$-greedy policy to learn the best strategy to play.\n",
    "\n",
    "The following wikipedia pages was used as a reference \n",
    "[Q learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "\n",
    "$$\n",
    "Q^{n e w}\\left(s_{t}, a_{t}\\right) \\leftarrow \\underbrace{Q\\left(s_{t}, a_{t}\\right)}_{\\text {old value }}+\\underbrace{\\alpha}_{\\text {learning rate }} \\cdot \\overbrace{(\\underbrace{r_{t}}_{\\text {reward }}+\\underbrace{\\gamma}_{\\text {discount factor }} \\cdot \\underbrace{\\max _{a} Q\\left(s_{t+1}, a\\right)}_{\\text {estimate of optimal future value }}-\\underbrace{Q\\left(s_{t}, a_{t}\\right)}_{\\text {old value }})}^{\\text {temporal difference }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.99 # discount factor\n",
    "# epsilon is the exploration rate\n",
    "min_epsilon = 0.01\n",
    "max_epsilon = 1\n",
    "espilon_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoinGameSimulation()\n",
    "shape_of_state = tuple(d.n for d in env.observation_space[:-1])\n",
    "Q_table = np.zeros((*shape_of_state, env.action_space.n))\n",
    "scores = []\n",
    "rewards = []\n",
    "num_trained_episodes = 0\n",
    "av_rewards = []\n",
    "\n",
    "def epslion_greedy(state):\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 0\n",
    "rewards = []\n",
    "\n",
    "for itr in tqdm(range(num_episodes)):\n",
    "    env.reset()\n",
    "    (n_heads, n_tails, n_flips_left) = env.observe()\n",
    "    state = (n_heads, n_tails)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        move = epslion_greedy(state)\n",
    "        (n_heads, n_tails, n_flips_left), reward, done, _  = env.step(move)\n",
    "        next_state = (n_heads, n_tails)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        Q_table[state][move] = Q_table[state][move] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state][move])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-espilon_decay*itr)\n",
    "\n",
    "    scores.append(env.score)\n",
    "    av_rewards.append(np.sum(rewards))\n",
    "    rewards = []      \n",
    "    num_trained_episodes += 1\n",
    "    env.reset()\n",
    "clear_output(wait=True)\n",
    "plt.plot(np.array(scores).reshape(1000,-1).mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q_table[1,0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc32a243a6b87027c9f6195a114662f388139f44cd834ded9ed282018985cc20"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('web-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
