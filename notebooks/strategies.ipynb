{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from game_simulation import CoinGameSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=CoinGameSimulation()\n",
    "g.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheaters are only biased against heads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_strategy(strategy, n_simulations=500):\n",
    "    # Simulate the game with the given strategy\n",
    "    # strategy is a function that takes n_heads, n_tails and flips_left and returns an action in (\"fair\", \"cheater\", \"one_flip\", \"five_flips\")\n",
    "    g = CoinGameSimulation()\n",
    "    scores = []\n",
    "    n_labels_list = []\n",
    "    n_flips_list = []\n",
    "    n_flips_per_label_list = []\n",
    "    rewards = []\n",
    "    for _ in range(n_simulations):\n",
    "        n_heads, n_tails, flips_left = g.reset()\n",
    "        n_labels = 0\n",
    "        n_flips = 0\n",
    "        n_flips_per_label = 0\n",
    "        done = g.done\n",
    "        while not done:\n",
    "            action = strategy(n_heads, n_tails, flips_left)\n",
    "            (n_heads, n_tails, flips_left), reward, done, _ = g.step(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if action == 2:\n",
    "                n_labels += 1\n",
    "                n_flips_per_label_list.append(n_flips_per_label)\n",
    "                n_flips_per_label = 0\n",
    "            elif action == 3:\n",
    "                n_labels += 1\n",
    "                n_flips_per_label_list.append(n_flips_per_label)\n",
    "                n_flips_per_label = 0\n",
    "            elif action == 0:\n",
    "                n_flips_per_label += 1\n",
    "                n_flips += 1\n",
    "            elif action == 1:\n",
    "                n_flips_per_label += 5\n",
    "                n_flips += 5\n",
    "            else:\n",
    "                raise ValueError(\"Unknown action: {}\".format(action))\n",
    "        scores.append(g.score)\n",
    "        n_labels_list.append(n_labels)\n",
    "        n_flips_list.append(n_flips)\n",
    "    \n",
    "    for name, l in zip([\"score\", \"n_labels\", \"n_flips_per_game\", \"n_flips_per_label\", \"reward\"],[scores, n_labels_list, n_flips_list, n_flips_per_label_list, rewards]):\n",
    "        print(name, np.mean(l), \"±\", np.std(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline strategy\n",
    "This is simply the random strategy to get a baseline of what to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 6.92 ± 7.423314623535768\n",
      "n_labels 14.088 ± 11.134821776750627\n",
      "n_flips_per_game 0.0 ± 0.0\n",
      "n_flips_per_label 0.0 ± 0.0\n",
      "reward -7.896081771720613 ± 22.496513490541393\n"
     ]
    }
   ],
   "source": [
    "simulate_strategy(lambda *_: random.choice([2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1\n",
    "This stategy uses the [Binomial proportion confidence interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval) to determine the the coin to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 38.0 ± 0.0\n",
      "n_labels 50.0 ± 0.0\n",
      "n_flips_per_game 340.0 ± 0.0\n",
      "n_flips_per_label 6.8 ± 7.691553809211765\n",
      "reward -0.6842105263157895 ± 10.4128090733377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Documents\\My_Code\\Primer_game\\game.py:83: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "z = st.norm.ppf(.90)\n",
    "p_fair = 0.5\n",
    "p_cheater = 0.9\n",
    "\n",
    "def strategy_binom_ci(n_heads, n_tails, flips_left):\n",
    "    n = n_heads + n_tails\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    p_hat = n_heads / n\n",
    "\n",
    "    p_hat_ci = z * np.sqrt(p_hat * (1 - p_hat) / n)\n",
    "\n",
    "    if p_hat - p_hat_ci < p_fair < p_cheater + p_hat_ci:\n",
    "        return 2\n",
    "    \n",
    "    if p_hat - p_hat_ci < p_cheater < p_cheater + p_hat_ci:\n",
    "        return 3\n",
    "    \n",
    "    if flips_left <= 0:\n",
    "        if abs(p_fair-p_hat) > abs(p_cheater-p_hat):\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    return 0\n",
    "\n",
    "simulate_strategy(strategy_binom_ci, n_simulations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: I try to reproduce my play.\n",
    "This strategy tries to mimic the way I play the game in my head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_mimic_attempt(n_heads, n_tails, flips_left):\n",
    "    n_throws = n_heads + n_tails\n",
    "    diff = (n_heads-n_tails)\n",
    "    if diff > 5:\n",
    "        return 3\n",
    "    elif diff < 0 or (n_throws > 10 and diff < 1):\n",
    "        return 2\n",
    "    if flips_left <= 0:\n",
    "        if diff > 4:\n",
    "            return 3\n",
    "        else:\n",
    "            return 2\n",
    "    return 0\n",
    "\n",
    "random.seed(1)\n",
    "simulate_strategy(strategy_mimic_attempt, n_simulations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: make use of Bayesian inference.\n",
    "This strategy uses Bayesian inference to determine the best strategy to play.\n",
    "\n",
    "The following wikipedia pages were used as reference \n",
    "[Checking whether a coin is fair](https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair) and [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference).\n",
    "\n",
    "The following code shows the probabilitic graphical model for the coin flip distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOIAAABtCAYAAAC89r3aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAS2ElEQVR4nO2deZhU1ZmH318VqBBAkWhslCWCmg6uLImOClGIJmp0xETNhIwaNDI4MuKuxGVCwmgkTByVwWRwC2gwMSHGLS4jUePWKBjjMi7QDRgMNMPWBkS6vvxxTmPRdnfVvXWr+kKf93nqebrq3vudr6rv757tO+eTmREIBNqXTHs7EAgEghADgVQQhBgIpIAgxEAgBQQhBgIpIAgxEEgBQYjbAJJOlrRUUoOkQxK2/S1Jj8a8VpJul7Ra0ouSviRpWd7x1yR9KSlft2eCEPOQVCtpg7/h/+pvsm55x78s6UlJ6yWtkrRQ0mWSdvLHr5VkkiY0s3uB//zamK5NBf7VzLqZ2YIW/JakCZL+LOkDScsk/VLSAYUMm9lsMzsmpl9HAF8G9jKzL7Rge5CZzYtpu0MRhPhJvmZm3YDBwDDgewCSvgH8Crgb6GdmvYDTgL2APnnXvwWc0czmP/vP49IPeK2N4zcC/wZMAHYF9gXmAseXUGaxftWa2QdlLmf7x8zCy7+AWmBU3vsbgAcAAUuBiwpcfy0wC3gDGOQ/G+TfzwKubeW6DE7wdcAK4C5gZ2BHoAEw4APg3Rau3QdoBL7Qhl87e5srfRnfAzL+2JnAM3nnGjAOeBtYDdwCqAWbY4GNvuwG4N+BLwHLWvo9/W/zK2AOsB54GTiovf/naXmFGrEVJPUBjgMWAPvhar77irz857haEFzteFeB88/0r6OAvYFuwM1m9qG52hncTTughWtH4m7+F9uwfxNOjHsDI7xvZ7Vx/gm41sBBwKnAsc1PMLOZOME+Z67JfE0b9po4Cfglrta+G5grqXMR1233BCF+krmS1gDPAH8ApgCf9sfebzpJ0i8krZH0N0nfbmZjFvBNf5Od7t+3xbeAaWa2yMwagCuA0yV1KsLfXsDy1g5KyuKa0FeY2XozqwV+DDT3OZ/rzGyNmS0BngQOLsKPYnjJzH5lZh8B04CdgEMTsr1NU8w/uqPxj2b2eP4Hklb5P6uAxQBmdro/9gyQzT/fzJZIegcn4rfNbKmktsrsjWsyNlGH+998BnivgL+rvF+t8Wlghxbs79nGNe/n/f03XA2dBEub/jCznB9h7Z2Q7W2aUCMWx5s4QYyOcM1dwEUUbpYC/AU38NFEX2Az8Ncirn0C2EvS0FaO1wMftWC/kMDLwZZBLUkZXHP/L+3gR+oIQiwCc6MNFwHXSDpHUk8/ZbAPrtZqiTnAMcC9RRRxDzBR0mf9dMkUYI6ZbS7Ct7eB6cA9fh5vB0k7STpd0uVm1uh9+KGk7pL6ARdSuLlcDoZIGu2b3BcAHwLPt4MfqSMIsUjMbA5u4GIMrolVj7vBf4obgGh+/gYze9zMNhRh/jbcAM9TuKbvRuD8CO5NAG7GjXCuAd4FTgZ+54+fjxt1XYTr+97ty6w0v8X1V1fj+qijfX+xwyM/tBwIlBUfzDDQzMa0ty9pJNSIgUAKCEIMBFJAaJoGAikg1IiBQAoIQgwEUkAQYiCQAoIQA4EUEIQYCKSAIMRAIAUEIQYCKSAsgwpUHEm1bL0aZHukzsz6F3tymNAPVBxJZmZtLtDc1on6HUPTNBBIAUGIgUAKCEIMBFJAEGIgkAKCEAOBFBCEGAikgCDEQCAFBCEGAikgCDEQSAFBiIFACghCDARSQOygb0m/wKUr6w/sAYw3swcT8qtdkbQDMBDojktTthpY5HfN3i6Q1JQdakfclvx/MbNWk9lsa8glGxkCVOOS3awD5plZMWkMKk4pqy8OAuaa2WmSjsBl99lmhShpX+AcXNqy/XG7ea/1h3cDPi1pAfAYMNPMtqmcDT7XxEhcSrYv4pK/vIvbVbwz0FfSJmA+Lo/hvWa2sZ3cjYWkzwBTs9ns8cAugDp37twIkMvl1NjYmMlkMpslLcnlctOA6ZaSVQ+xVl9I6gIsAfqY2UZJuwIvmNk+STtYbnzylim4B8ttuMSkC61ZFlxJPXFP2FNwqdYeAy43s0WV9TgavmY4E5fqbQNwK25r/zfzc2v48/rh0qSdAQwFZgBTikwbEMWnRFdfSNozk8k8kMvlDu7Ro8fmUaNGdTr88MMZOHAgmczHva+NGzcyf/585s2bZzU1NWZmjWZ2C3Bh0oKM+h3jCnEYcKOZ/YN/PwoYZ2Zfj2ysnZC0I3A1cDZwJTDLzD4s8toewHn4xDTAf5tZrly+xkVSX+BnuByKFwB/LPaGkzQQ+AFwCHCWmT2boF+JCVHSJcB1VVVVNnHixGx1dXVR1+VyOe677z5mz55tZrYil8uNNLO20qNH9asiQjwbl4p5IC434KPApWb2x8jG2gFfgz+IS3s2zszeL3BJa3Y+B9wOLAPGFCvkSiDpMGAucCPwo2IyS7Vi5xRccpurzOxnCfmWiBCz2ezDZnbsmDFjdOqpp8ay0dDQwDXXXNP41ltvZYCvm9mvS/ULKifEm3AJLEcAPXDNl/ZI8xUZP0jxv7hswBeV2iTxNes9uAfSKXFv+CSR9EVcJqgzzOzhBOwNBB7H/Z9/moC9koWYzWYflTRq6tSpGjhwYKkuceutt9oDDzwAcJKZ/a7Q+YWolBCfAs4xs/+LfHE74vtBc3FJOs9Lql/gR1nvB141s0uSsFmCL7sDrwBnJzmK7cX4NHCamT1Voq2ShCjpPyRdNm3atERE2MQtt9xijzzyiAF7lTqCXCkhvocbqEldv6gtfK77S4ChZrYpYdu7A38CTjaz55K0HcEH4XI1vmtml5XB/om40fGDmg9mRbQTW4j+gfDWueeeqxNOOCGuC61yzjnnNK5YseLdxsbG/UqxU5GtMsxsz21QhLsCPwbOTFqEAGa2ApcQdKakbNL2i+REYBBuAClxzOx+4Lly2S+GTCbz+/79++fKIUKAKVOmZM1sH0nnlaWAVuhIkTVnAb83s5fLVYCZ/RJoAL5SrjIKcCFuUKWc839XAGN9ivGKImlQLpfbe9KkSWV70O22224MHz6cTCZzdbnKaIkOIUQ/mf0vuNG/cjMdGF+BcrZC0v64UezflrMcM1uGG+j6p3KW0xKSflJVVdW4xx57lLWcsWPHKpfL7e7nmCtChxAiMBgXxvVCBcqaAwz3o7OV5FTg5xXKSX87Lqih0gwfPXp0wdrw4YcfZvr06Vt9Nn78eJYuXVpUIT179qSqqqoRmBTLyxjEEqKkHSWdLOksSSV1aivEUOC5KKOkks6VFLkG9VEof8KJv5IMw/XfikZSVtKNkl6T9KqkvYu89DlgiG9pVARJPc1sh+HDhxc8t7a2lgEDBmx5v2nTJurr6+ndu3fR5VVXV2ez2eyQWM7GIPIPKakr8CJwF3AT8LKkk5J2LCqSjvQPh75+9DCfIcBLEU0eCLwa052XfJmJISkj6ZuSjmpe2+YFOEf9jlfggtkHAf9FkU1qM6vHBcIPKHRuFCT1kvQdSYP9lFA+ozt16pTr2rVrQTt1dXVbCbG2tpbevXuTzRbftRw2bBi5XK6q6AtKJE7Q90RgX1xEexN3+yiT9gyg/TXQBfdwaZT0Kq4v8yKwjz8ehQOAu2P6sgjYX9JeMa9vib2BWcB6oKukVcDLwDzcvOEuuPnRopD0KdxUS9MDYzFwfAR/aoFDJMWKQ23ltzkfd39tBHbyW/M/C/wRGNGtW7ccRVQeS5YsYcqUKTQ9jzds2MCwYcMi+VddXY2ZVSwlRZyCPsfWIgQngCWlu5Moh/kXuGDnqOFn+wN/jln2JmCsfyVNU224B3CcfwF8FDFAYRTQR9JC/35XXPRMseyG6w/HpbUOm+GWn4F7gO6DC0Inm80WXIa2cuVKevTowYwZM7Z8NmPGDKIO8HTp0iXS+aUSp43/Ei68LZ/VQMbM1F4voM779RFuec9s3EjpF4Bn+OTDo1Uk9QHWm9nagie3zE64oPgkv98AoBH4APdgeRkXR/pN3NyhWmiSt8XBwNVmdrCZHYyLF14Y4fqVwNExvwutfH4h7v+3Cfh/4AlcYP5XgRmbN28u+KCpra2lb9++W322dOlS+vXrR319PVdeeSVz587l+uuvb9POBx/EjleIRZwa8RbcD3M4sBkn5pOSChcrga8A3XBhZlvVfpLewTWnHyrSVin9Q3xZr5dw/Scws0WSjgLex0XObBVQIemvuEXai4s02bPpXEmdgGOAH0Zwad8IZRXLDFwrZKGZrcw/IKlXQ0PDdwsZaEmIS5YsoX///rzzzjsceuihnHjiiUydOrVNO6+88gp+fWZFiFwj+uHxrwJHAd8ABpjZM0k7FhUze9PM5jcXoSfq4MkBlCbEOAMnBTGzp83s7VaimqJ+x7dwaw/B9cseNLOihCWpCreyvy5CeQUxsw1m9lhzEXrub2xszKxd23Yjpa6ujj59+mx5v379esyMnj17snjxYg455BAACjUeFixYgKRlkb9ETOKGuOXMrKaNHy1tvAgcEWG4/QDgu5Jq/avoaQG/VrGaaM28JHgRODLC+fcAg31r4UBcs7BYjgBqKtkKMrP1mUxm4xNPPNHmeRdffDEjRozY8r579+7MmuUWBi1fvpw999yTtWvXsssuu7Rp5/XXX2/M5XKVmHcGOkh+RN93egW3EjvKgEScss4DRphZvAVy8csdADwP9LWEV9S3UNajwJ1mNjvm9ZECovOu+3WvXr2+dscdd5R1NPO9995j3LhxANVm9mYcGxUJ+t7W8E/u6bhV9WXDC368L6uimNm7QA1wWjnLkdvb5yDcvjaVZuKqVas6LV6cdNd0a2bOnGmZTGZJXBHGoUMI0TMLGOoHPMrFd3HzfH8oYxlt8SPg++UKr/MPmv/EjQhXfDcCM6vLZDILJ0+eXLbd9BYvXkxNTY1yudyl5SqjJTqMEM2sATgXt0wp8ZUDkvrj9nj5TnuNIJvZPOBhoO0hwficgdv97YYy2S9ILpc7pr6+fku/L2HbXHXVVY2ZTOZ5MytljjQyHUaIAGb2EPAkcKcfsk8ESd2Be4EbzCzRaYsYXAKMlHRWkkYlDcHVuGdaZQLLW8TMVprZBXPmzKGmpiZR25MnT86tW7euMZfLHZOo4SLoUEL0jMfNN/5cUudSjUnaBVcLLaAda4omzGwdbk71h0mJUW7Xvgdx26O8koTNUjCzmyXNnDx5sr3wQjIDm5MnT87Nnz8fMzvczNYnYjQCHWLUtDly+7LeA1ThnvBvxLRzJG4v1IeAia3M77ULflXMI/51aZyby0/3nA9chdtSseRNlbzdWKOmLdiZAZw7cuRImzBhgvL3MC2W5cuXM2nSpMb6+nozsyPN7PlS/fK+RfqOHVKIsOUmGwd8H7eKZHqxc6K+P3gxMBqXamBumdwsCV9bT8MFX1wO/MaK2CbED8ocievzZnAifDtBvxIRord1iqTZXbt27XT22Wdnjz76aIoRZENDA7fddhuPP/64Sfqz39c0sTnxIMSISPos7ol/Mm6X7wdxUSpbwsh8f7Kaj3f6Pgy4E7e94Kr28DsKko7BCbEat6j3KeCl/BvPD2AdjIu2ORO3PeRPgP+xhHN+JClEb6+LpF+Y2fGdO3fW0KFDM4MHD+awww5j550/HkBetGgRzz77LDU1NY2LFi3KZjKZ1blc7ipzu30nShBiTPzmUt/G7dU6BLc79jpcop2dcasFXsJttT/HzJoHvqceSZ/Hfccv4hYu53AB5J2BTwGv4b7jvbiELWW5OZIWYp7dLDApk8mMMbO+ZrZj8/IymcxaM3vVzK42syeT9iHPlyDEJPChat1xN+s6K2H7wDTim+a74laKbAJWV2o0tFxCbKGcT+EC4bsAa3CtnIrc8EGIgdRTKSG2JyHELRDYBglCDARSQBBiIJACghADgRQQhBgIpIAgxEAgBQQhBgIpIAgxEEgBQYiBQAoIQgxsl0i6TdIKSZF2a/fJh0xSdd5nb/gVN2UjCDGwvXIH8RLGHojbCvN4cJnPgM+Q8B6uzQlCDKQKSb+R9ANJT0t6X9KoOHbM7Cnctv1ROQC4jo8T8gwC3ih3sHgQYiBt7A+sMbMjcduafCv/oBfowhZekQQr6SFJLSVM/DxwP7C73w2v1F3fi6JiaacCgULI5d7cGbdlI7j7c03+OV6gJWNmxzX/zCcfWmVmGyQ9BhyLa6r+KYky2yIIMZAmBuF2DmjaEeBAmqXGk/Q0H6dty+fiBHZxz08+9BCuNq4C5pZotyBBiIE0sT9b5ww5EPht/glJ1YitkN8M/QMuO1VXKtA0DX3EQJo4gK2FGDtZrKR7gOeA/SQtkzS22fGW+ohbhOh3Mn8V2GRma+L4EMnfsEI/UGnCCv1PEmrEQCAFBCEGAikgCDEQSAFBiIFACghCDARSQJhHDLQHdZK29+H6SEHiYfoiEEgBoWkaCKSAIMRAIAUEIQYCKSAIMRBIAUGIgUAKCEIMBFJAEGIgkAKCEAOBFBCEGAikgCDEQCAFBCEGAing7wh+qO0Yv38BAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 212.126x79.3701 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import daft\n",
    "\n",
    "pgm = daft.PGM()\n",
    "pgm.add_node(\"p\", r\"$p$\", 0, 0, fixed=True, offset=(1, 5))\n",
    "pgm.add_node(\"L\", r\"$L$\", 1, 0)\n",
    "pgm.add_node(\"theta\", r\"$\\theta$\", 2, 0)\n",
    "pgm.add_node(\"H_n\", r\"$H_n$\", 3, 0, observed=True)\n",
    "pgm.add_edge(\"p\", \"L\")\n",
    "pgm.add_edge(\"L\", \"theta\")\n",
    "pgm.add_edge(\"theta\", \"H_n\")\n",
    "pgm.add_plate([2.5, -0.5, 1, 1], label=r\"$n = 1:N$\", shift=-0.2)\n",
    "ax = pgm.render()\n",
    "_ = ax.set_title(\"PGM of Coin flip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Distributions:\n",
    "- $L \\sim$ Bernouilli distribution with unkown parameter p. Determines if the opponent is a cheater or not.\n",
    "- $\\theta \\sim$ Unkown distribution with values $\\in [0, 1]$. Gives parameters of the Bernoulli distribution of H.\n",
    "- $H_{1:N}$: Bernouilli distribution with parameter $\\theta$ and $N$ samples. Gives the number of heads.\n",
    "\n",
    "$$P(L | H_{1:N}) = \\frac{P(H_{1:N} | L) \\cdot P(L)}{P(H_{1:N})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = CoinGameSimulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reset()\n",
    "\n",
    "class StrategyBayseianInference:\n",
    "\n",
    "    def __init__(self, n = 100, p_value=0.9):\n",
    "        self.n = n      # how many values of theta we include\n",
    "        self.p = 0.5\n",
    "        self.thetha_L = np.ones((2, n)) / n\n",
    "\n",
    "        self.p_value = p_value\n",
    "        self.last_n_heads = 0\n",
    "        self.last_n_tails = 0\n",
    "    \n",
    "    def __reset(self):\n",
    "        n = self.n\n",
    "        self.prior.fill(1/n)\n",
    "        self.last_n_heads = 0\n",
    "        self.last_n_tails = 0\n",
    "\n",
    "    def strategy(self, n_heads, n_tails, flips_left):\n",
    "        \n",
    "        n_new_heads = n_heads - self.last_n_heads\n",
    "        n_new_tails = n_tails - self.last_n_tails\n",
    "\n",
    "        assert max(n_new_heads, n_new_tails) <= 1, f\"{n_new_heads}, {n_new_tails}\"\n",
    "        assert min(n_new_heads, n_new_tails) == 0, f\"{n_new_heads}, {n_new_tails}\"\n",
    "\n",
    "        theta = self.theta\n",
    "        probability_of_observation = theta ** n_new_heads * (1 - theta) ** n_new_tails\n",
    "\n",
    "        posterior = probability_of_observation * self.prior\n",
    "        posterior /= np.sum(posterior)\n",
    "        self.prior = posterior\n",
    "\n",
    "        self.last_n_heads = n_heads\n",
    "        self.last_n_tails = n_tails\n",
    "\n",
    "        if n_heads + n_tails < 5:\n",
    "            return 0\n",
    "\n",
    "        p_fair = posterior[0.3 < theta].sum()\n",
    "        p_cheater = posterior[0.5 < theta].sum()\n",
    "        if p_fair > self.p_value:\n",
    "            self.__reset()\n",
    "            return 2\n",
    "        if p_cheater > self.p_value:\n",
    "            self.__reset()\n",
    "            return 3\n",
    "\n",
    "        if flips_left <= 0:\n",
    "            self.__reset()\n",
    "            if p_fair > p_cheater:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "        \n",
    "        return 0\n",
    "\n",
    "\n",
    "strat = StrategyBayseianInference(p_value=0.81, n=100)\n",
    "simulate_strategy(strat.strategy, n_simulations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = CoinGameSimulation()\n",
    "n = 100\n",
    "prior = np.ones(n) / n\n",
    "theta = np.linspace(0, 1, n)\n",
    "p_value = 0.9\n",
    "\n",
    "(n_heads, n_tails, flips_left), reward, done, _ = g.step(1)\n",
    "last_n_heads, last_n_tails = 0,0\n",
    "done = g.done\n",
    "while not done:\n",
    "    n_new_heads = n_heads - last_n_heads\n",
    "    n_new_tails = n_tails - last_n_tails\n",
    "\n",
    "    probability_of_observation = theta ** n_new_heads * (1 - theta) ** n_new_tails\n",
    "\n",
    "    posterior = probability_of_observation * prior\n",
    "    posterior /= np.sum(posterior)\n",
    "    prior = posterior\n",
    "\n",
    "    p_fair = posterior[(0.3 < theta)&(theta < 0.6)].sum()\n",
    "    p_cheater = posterior[(0.5 < theta)&(theta < 1)].sum()\n",
    "\n",
    "    n_thows = n_heads + n_tails\n",
    "    print(f\"{p_fair=} \\n {p_cheater=} \\n {flips_left=} \\n {n_thows=} \\n {n_heads=} \\n {n_tails=}\")\n",
    "    plt.plot(theta, posterior)\n",
    "    plt.show()\n",
    "    time.sleep(2)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    last_n_heads = n_heads\n",
    "    last_n_tails = n_tails\n",
    "\n",
    "    (n_heads, n_tails, flips_left), reward, done, _ = g.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reset()\n",
    "for _ in range(20):\n",
    "    (n_heads, n_tails, flips_left), reward, done, _ = g.step(1)\n",
    "    time.sleep(0.1)\n",
    "    print(f\"{n_heads=}, {n_tails=}, {flips_left=}, {reward=}            \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n_heads, n_tails, flips_left), reward, done, _ = g.step(1)\n",
    "print(f\"{n_heads=}, {n_tails=}, {flips_left=}, {reward=} {done=}   \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 4: make use of Q-learning.\n",
    "This strategy uses Q-learning with an $\\epsilon$-greedy policy to learn the best strategy to play.\n",
    "\n",
    "The following wikipedia pages was used as a reference \n",
    "[Q learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "\n",
    "$$\n",
    "Q^{n e w}\\left(s_{t}, a_{t}\\right) \\leftarrow \\underbrace{Q\\left(s_{t}, a_{t}\\right)}_{\\text {old value }}+\\underbrace{\\alpha}_{\\text {learning rate }} \\cdot \\overbrace{(\\underbrace{r_{t}}_{\\text {reward }}+\\underbrace{\\gamma}_{\\text {discount factor }} \\cdot \\underbrace{\\max _{a} Q\\left(s_{t+1}, a\\right)}_{\\text {estimate of optimal future value }}-\\underbrace{Q\\left(s_{t}, a_{t}\\right)}_{\\text {old value }})}^{\\text {temporal difference }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.99 # discount factor\n",
    "epsilon = 0.05 # exploration rate\n",
    "min_epsilon = 0.01\n",
    "max_epsilon = 1\n",
    "espilon_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoinGameSimulation()\n",
    "shape_of_state = tuple(d.n for d in env.observation_space[:-1])\n",
    "Q_table = np.zeros((*shape_of_state, env.action_space.n))\n",
    "scores = []\n",
    "rewards = []\n",
    "num_trained_episodes = 0\n",
    "av_rewards = []\n",
    "\n",
    "def epslion_greedy(state):\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 0\n",
    "rewards = []\n",
    "\n",
    "for itr in tqdm(range(num_episodes)):\n",
    "    env.reset()\n",
    "    (n_heads, n_tails, n_flips_left) = env.observe()\n",
    "    state = (n_heads, n_tails)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        move = epslion_greedy(state)\n",
    "        (n_heads, n_tails, n_flips_left), reward, done, _  = env.step(move)\n",
    "        next_state = (n_heads, n_tails)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        Q_table[state][move] = Q_table[state][move] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state][move])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-espilon_decay*itr)\n",
    "\n",
    "    scores.append(env.score)\n",
    "    av_rewards.append(np.sum(rewards))\n",
    "    rewards = []      \n",
    "    num_trained_episodes += 1\n",
    "    env.reset()\n",
    "clear_output(wait=True)\n",
    "plt.plot(np.array(scores).reshape(1000,-1).mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Q_table[1,0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc32a243a6b87027c9f6195a114662f388139f44cd834ded9ed282018985cc20"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('web-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
